import asyncio
import os
import pandas as pd
from pathlib import Path
from sqlalchemy import create_engine
from typing import List

# Imports de tes modules existants
from ingestion.table_extractor import PDFElectionExtractor
from ingestion.clean_data import ElectionDataCleaner
from database.connection import DatabaseConnection
from database.schema import metadata, creation_views_sql

class ElectionLoader:
    """
    Orchestrateur ETL (Extract, Transform, Load).
    1. Extrait les donn√©es du PDF via LlamaCloud.
    2. Nettoie les donn√©es via Pandas.
    3. Sauvegarde en CSV (backup).
    4. Ins√®re les donn√©es normalis√©es dans la base SQLite.
    """

    def __init__(self, pdf_path: str, db_path: str, output_csv_path: str):
        self.pdf_path = Path(pdf_path)
        self.db_path = Path(db_path)
        self.output_csv_path = Path(output_csv_path)
        
        # Initialisation des composants
        api_key = os.getenv("LLAMA_CLOUD_API_KEY")
        if not api_key:
            raise ValueError("La cl√© LLAMA_CLOUD_API_KEY est manquante dans le .env")
            
        self.extractor = PDFElectionExtractor(api=api_key)
        self.cleaner = ElectionDataCleaner()

    async def run_pipeline(self):
        """
        Ex√©cute tout le pipeline de donn√©es.
        """
        print(f" D√©marrage du pipeline pour : {self.pdf_path.name}")


        # 1. Extraction (Appel API)
        raw_data = await self.extractor.extract_from_pdf(str(self.pdf_path))
        
        # 2. Aplatissement
        df_raw = self.extractor._flatten_to_dataframe(raw_data)
        
        # 3. Nettoyage
        print("üßπ Nettoyage des donn√©es...")
        df_clean = self.cleaner.clean(df_raw)
        
        # 4. Sauvegarde CSV (Checkpoint)
        self.output_csv_path.parent.mkdir(parents=True, exist_ok=True)
        df_clean.to_csv(self.output_csv_path, index=False)
        print(f" Donn√©es nettoy√©es sauvegard√©es : {self.output_csv_path}")

        # ---------------------------------------------------------
        # √âTAPE 2 : CR√âATION DES TABLES
        # ---------------------------------------------------------
        print(" Initialisation de la base de donn√©es...")
        
        # On utilise SQLAlchemy juste pour cr√©er les tables proprement selon le schema.py
        # (L'URI sqlalchemy n√©cessite 3 slashs pour un chemin relatif ou 4 pour absolu)
        engine_url = f"sqlite:///{self.db_path.resolve()}"
        engine = create_engine(engine_url)
        
        # On vide les anciennes tables si elles existent (Reset)
        metadata.drop_all(engine)
        metadata.create_all(engine)
        print("Tables cr√©√©es (Circonscriptions & Candidats).")

        # ---------------------------------------------------------
        # √âTAPE 3 : INSERTION DES DONN√âES
        # ---------------------------------------------------------
        print(" Insertion des donn√©es dans SQLite...")
        
        # On utilise ta classe de connexion pour l'insertion
        # IMPORTANT : read_only=False car on veut √©crire
        db_conn = DatabaseConnection(str(self.db_path), read_only=False)
        conn = db_conn.get_connection()
        cursor = conn.cursor()

        try:
            # On groupe par circonscription pour ins√©rer d'abord le parent, puis les enfants
            # Cl√©s uniques pour identifier une circonscription (AVEC COLONNES NORMALIS√âES)
            cols_group = [
                'region_nom', 'region_nom_norm', 'code_circonscription', 
                'nom_circonscription', 'nom_circonscription_norm',
                'nb_bureaux_vote', 'inscrits', 'votants', 'taux_participation',
                'bulletins_nuls', 'suffrages_exprimes', 
                'bulletins_blancs_nombre', 'bulletins_blancs_pourcentage'
            ]

            grouped = df_clean.groupby(cols_group)

            for group_keys, df_group in grouped:
                # 1. Pr√©parer les donn√©es de la Circonscription (Parent)
                # group_keys contient les valeurs dans l'ordre de cols_group
                data_circo = dict(zip(cols_group, group_keys))
                
                # Renommage pour matcher le SQL (nb_bureaux_vote -> nb_bureau)
                data_circo['nb_bureau'] = data_circo.pop('nb_bureaux_vote')
                
                # Insertion Circonscription (AVEC COLONNES NORMALIS√âES)
                sql_circo = """
                    INSERT INTO circonscriptions 
                    (region_nom, region_nom_norm, code_circonscription, 
                     nom_circonscription, nom_circonscription_norm, 
                     nb_bureau, inscrits, votants, taux_participation, 
                     bulletins_nuls, suffrages_exprimes, 
                     bulletins_blancs_nombre, bulletins_blancs_pourcentage)
                    VALUES (:region_nom, :region_nom_norm, :code_circonscription, 
                            :nom_circonscription, :nom_circonscription_norm, 
                            :nb_bureau, :inscrits, :votants, :taux_participation, 
                            :bulletins_nuls, :suffrages_exprimes, 
                            :bulletins_blancs_nombre, :bulletins_blancs_pourcentage)
                """
                # Le dictionnaire data_circo contient d√©j√† les colonnes _norm
                # car elles sont dans le df_clean venant du cleaner !
                cursor.execute(sql_circo, data_circo)
                
                # R√©cup√©rer l'ID g√©n√©r√© pour cette circonscription
                circo_id = cursor.lastrowid

                # 2. Ins√©rer les Candidats (AVEC COLONNES NORMALIS√âES)
                candidates_data = []
                for _, row in df_group.iterrows():
                    candidates_data.append({
                        "circonscription_id": circo_id,
                        "nom_liste_candidat": row["nom_liste_candidat"],
                        "nom_liste_candidat_norm": row["nom_liste_candidat_norm"], # Ajout
                        "parti_politique": row["parti_politique"],
                        "parti_politique_norm": row["parti_politique_norm"],       # Ajout
                        "score_voix": row["score_voix"],
                        "pourcentage_voix": row["pourcentage_voix"],
                        "est_elu": 1 if row["est_elu"] else 0
                    })
                
                sql_candidat = """
                    INSERT INTO candidats 
                    (circonscription_id, nom_liste_candidat, nom_liste_candidat_norm, 
                     parti_politique, parti_politique_norm, 
                     score_voix, pourcentage_voix, est_elu)
                    VALUES (:circonscription_id, :nom_liste_candidat, :nom_liste_candidat_norm, 
                            :parti_politique, :parti_politique_norm, 
                            :score_voix, :pourcentage_voix, :est_elu)
                """
                cursor.executemany(sql_candidat, candidates_data)

            conn.commit()
            print(f" Insertion termin√©e : {len(df_clean)} candidats trait√©s.")


            print(" Cr√©ation des vues analytiques...")
            for view_sql in creation_views_sql:
                cursor.execute(view_sql)
            conn.commit()
            print(" Vues cr√©√©es avec succ√®s.")

        except Exception as e:
            conn.rollback()
            print(f" Erreur critique lors de l'insertion : {e}")
            raise e
        finally:
            conn.close()


if __name__ == "__main__":
    # Configuration des chemins
    BASE_DIR = Path(__file__).resolve().parent.parent.parent # Remonte √† la racine du projet
    PDF_FILE = BASE_DIR / "data/raw/resultat.pdf"
    DB_FILE = BASE_DIR / "data/processed/elections.db" # Ou dans data/database/elections.db
    CSV_OUT = BASE_DIR / "data/processed/elections_clean.csv"

    # V√©rification que le PDF existe
    if not PDF_FILE.exists():
        print(f"Erreur : Le fichier {PDF_FILE} n'existe pas.")
        exit(1)

    # Lancement du loader
    loader = ElectionLoader(str(PDF_FILE), str(DB_FILE), str(CSV_OUT))
    
    # Ex√©cution asynchrone
    asyncio.run(loader.run_pipeline())